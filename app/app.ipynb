{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a4679a0023a64ebeb463b5e1a200f63c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ab8f59d3ecd434496bc150f381fedf3",
              "IPY_MODEL_408ae46a93ce4d5495b16bdd8d1b49b4",
              "IPY_MODEL_5201d563096341a9be104a0727edc282"
            ],
            "layout": "IPY_MODEL_1664b1799c1844e482854b7b41af674f"
          }
        },
        "6ab8f59d3ecd434496bc150f381fedf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a5074f1605243edafd00eda731a44fd",
            "placeholder": "​",
            "style": "IPY_MODEL_35b24d75e7354923bbbdfabe2737475b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "408ae46a93ce4d5495b16bdd8d1b49b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a823df219a54315859de2b553115e9b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3e6b08ca5ac4bf69570bf8b7aa92f0e",
            "value": 2
          }
        },
        "5201d563096341a9be104a0727edc282": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06dbfd9722f548b8a928bed2613d806e",
            "placeholder": "​",
            "style": "IPY_MODEL_48e4e89cfa8045ffb4d8d6c4ae05dfa0",
            "value": " 2/2 [00:02&lt;00:00,  1.07it/s]"
          }
        },
        "1664b1799c1844e482854b7b41af674f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a5074f1605243edafd00eda731a44fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35b24d75e7354923bbbdfabe2737475b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a823df219a54315859de2b553115e9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3e6b08ca5ac4bf69570bf8b7aa92f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06dbfd9722f548b8a928bed2613d806e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48e4e89cfa8045ffb4d8d6c4ae05dfa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_core langgraph streamlit\n",
        "!pip install -U transformers accelerate bitsandbytes torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCklYh6xSgrU",
        "outputId": "46e221e1-e36b-46f9-f196-03ce36d615ca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.5)\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.52.2-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core) (0.12.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.13.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.12.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain_core) (0.25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain_core) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (4.12.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.30.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.52.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m139.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.52.2\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Collecting torch\n",
            "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch)\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Downloading bitsandbytes-0.49.0-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m142.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, bitsandbytes\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.49.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 triton-3.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n"
      ],
      "metadata": {
        "id": "5tVN9zMDfEXb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7b7SBcixxbN",
        "outputId": "380c7d6e-e0e4-4b3a-a97c-29ab1449a6b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from typing import TypedDict, List, Dict, Union\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "# from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "# from dotenv import load_dotenv\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# from google.colab import userdata\n",
        "# HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=torch.bfloat16,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "\n",
        "\n",
        "def get_patient_info(patient_id):\n",
        "  # conn = sqlite3.connect(\"patient_clients.db\")\n",
        "  # cursor = conn.cursor()\n",
        "\n",
        "  # cursor.execute(\"SELECT * FROM bio WHERE patient_id = 'patient_id', (50,) \")\n",
        "  # row = cursor.fetchall()\n",
        "  row = \"35 year old Male\"\n",
        "\n",
        "  # conn.close()\n",
        "  return row\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# latest_patient_response = []\n",
        "\n",
        "counter = 0\n",
        "patient_id = \"\"\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: List[Dict]\n",
        "  running_notes: str\n",
        "  current_question: str\n",
        "  patient_response: str\n",
        "  system_questions: List[Dict]\n",
        "  differentials_list: str\n",
        "  plan: str\n",
        "  # more_info: bool\n",
        "\n",
        "def get_profile_info(state: AgentState)->AgentState:\n",
        "  \"\"\"This function is a node that gets the patient demographic information and medical history from the database.\"\"\"\n",
        "  pt_info = get_patient_info(st.session_state.patient_id)\n",
        "  state['messages'].append({\"role\":\"user\", \"content\":pt_info})\n",
        "  state['running_notes'] = pt_info\n",
        "  return state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def decide_questions_to_ask(state: AgentState)->AgentState:\n",
        "  \"\"\"This function decides which question to ask using an LLM\"\"\"\n",
        "  response = generator(state['messages'], max_new_tokens=256)[0][\"generated_text\"][-1][\"content\"]\n",
        "  state['system_questions'].append({response})\n",
        "  state['current_question'] = response\n",
        "  return state\n",
        "\n",
        "\n",
        "def ask_question_get_answer(state: AgentState)->AgentState:\n",
        "  \"\"\"This funtion asks the questions to the patient and gets the answers\"\"\"\n",
        "  # patient_response = st.text_area(\"Write you answers here:\", height=180, placeholder=\"Your answers here: \")\n",
        "  # latest_patient_response.append(patient_response)\n",
        "  if len(latest_patient_response)>0:\n",
        "    state['patient_response'] = latest_patient_response[-1]\n",
        "    latest_patient_response.pop(0)\n",
        "  return state\n",
        "\n",
        "\n",
        "def compile_notes(state: AgentState)->AgentState:\n",
        "  \"\"\"This combines the current medical history available in the current notes with the answer that the patient has just provided\"\"\"\n",
        "  compile_notes_prompt = f\"As an expert physician that pays attention to detail. Combine the following history {state['running_notes']} and these answers {state['patient_response']} provided by the patient to these questions {state['current_question']}\"\n",
        "  state['running_notes'] = generator([{\"role\":\"user\", \"content\": compile_notes_prompt}], max_new_tokens=256)[0][\"generated_text\"][-1][\"content\"]\n",
        "  return state\n",
        "\n",
        "\n",
        "def is_more_info_needed(state: AgentState)->bool:\n",
        "  \"\"\"Decide if this info is needed to be able to make a good plan for the patient\"\"\"\n",
        "  # global counter\n",
        "  prompt_full_history = f\"Answer with 'Yes' or 'No' only. Is this medical history detailed enough to make a good differential diagnosis and then a plan? {state['running_notes']}\"\n",
        "  if st.session_state.counter > 10:\n",
        "    return \"No\"\n",
        "  else:\n",
        "    st.session_state.counter += 1\n",
        "    decision = generator([{\"role\":\"user\", \"content\": prompt_full_history}], max_new_tokens=256)[0][\"generated_text\"][-1][\"content\"]\n",
        "    print(f\"This is the decision on whether more information is needed: {decision}\")\n",
        "    if decision.lower() is True:\n",
        "      return \"Yes\"\n",
        "    else:\n",
        "      return \"No\"\n",
        "\n",
        "def list_possible_differentials(state: AgentState)->AgentState:\n",
        "  \"\"\"This function creates a list of differential diagnosis based on the list\"\"\"\n",
        "  system_prompt_for_differential = f\"You are an expert physician, you have to provide a differential diagnosis for the provided medical history\"\n",
        "  user_prompt_for_differential = state['running_notes']\n",
        "  state['differentials_list'] = generator([{\"role\": \"system\", \"content\":system_prompt_for_differential},{\"role\":\"user\", \"content\":user_prompt_for_differential}], max_new_tokens=256)[0][\"generated_text\"][-1][\"content\"]\n",
        "  return state\n",
        "\n",
        "\n",
        "def create_plan(state: AgentState)->AgentState:\n",
        "  # Need RAG with a directory of physicians\n",
        "  \"\"\"This function creates a plan based on the history and the differential diagnosis list\"\"\"\n",
        "  system_prompt_for_plan = f\"You have to provide medical advise to this individual. Always insist that the patient needs to seek medical advice. Advise which specialty the patient needs to see. Do not provide definitive medical treatment but suggest what the patient should expect when they go to see a physician\"\n",
        "  user_prompt_for_plan = state['running_notes'] + state[\"differentials_list\"]\n",
        "  state['plan'] = generator([{\"role\": \"system\", \"content\":system_prompt_for_plan},{\"role\":\"user\", \"content\":user_prompt_for_plan}], max_new_tokens=256)[0][\"generated_text\"][-1][\"content\"]\n",
        "  return state\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"get_profile_info\", get_profile_info)\n",
        "graph.add_node(\"decide_questions_to_ask\", decide_questions_to_ask)\n",
        "graph.add_node(\"ask_question_get_answer\", ask_question_get_answer)\n",
        "graph.add_node(\"compile_notes\", compile_notes)\n",
        "graph.add_node(\"is_more_info_needed\", lambda state:state)\n",
        "graph.add_node(\"list_possible_differentials\", list_possible_differentials)\n",
        "graph.add_node(\"create_plan\", create_plan)\n",
        "\n",
        "\n",
        "graph.add_edge(START, \"get_profile_info\")\n",
        "graph.add_edge(\"get_profile_info\", \"decide_questions_to_ask\")\n",
        "graph.add_edge(\"decide_questions_to_ask\", \"ask_question_get_answer\")\n",
        "graph.add_edge(\"ask_question_get_answer\", \"compile_notes\")\n",
        "graph.add_edge(\"compile_notes\", \"is_more_info_needed\")\n",
        "graph.add_conditional_edges(\n",
        "    \"is_more_info_needed\",\n",
        "    is_more_info_needed,\n",
        "    {\n",
        "       \"Yes\": \"decide_questions_to_ask\",\n",
        "       \"No\": \"list_possible_differentials\",\n",
        "    }\n",
        ")\n",
        "\n",
        "graph.add_edge(\"list_possible_differentials\", \"create_plan\")\n",
        "graph.add_edge(\"create_plan\", END)\n",
        "agent = graph.compile()\n",
        "\n",
        "\n",
        "import streamlit as st\n",
        "# conversation_history = []\n",
        "if \"conversation_history\" not in st.session_state:\n",
        "  st.session_state.conversation_history = []\n",
        "\n",
        "if \"questions\" not in st.session_state:\n",
        "  st.session_state.questions = []\n",
        "if \"latest_patient_response\" not in st.session_state:\n",
        "  st.session_state.latest_patient_response = []\n",
        "\n",
        "if \"current_question\" not in st.session_state:\n",
        "  st.session_state.current_question = []\n",
        "\n",
        "if \"latest_patient_response\" not in st.session_state:\n",
        "  st.session_state.latest_patient_response = []\n",
        "if \"patient_id\" not in st.session_state:\n",
        "  st.session_state.user_id = \"1\"\n",
        "\n",
        "if \"counter\" not in st.session_state:\n",
        "  st.session_state.counter = 0\n",
        "\n",
        "# conversation_history.append(HumanMessage(content=\"You are an expert pysician taking history from a new patient. You have to ask more questions to get full details of the illness. Gather the information systematically in a step by step fashion\"))\n",
        "# questions = []\n",
        "st.header(\"Thuso App\")\n",
        "st.subheader(f\"May you please let me know what is your main concern today\")\n",
        "# user_input = input(\"Enter: \")\n",
        "if user_input := st.chat_input(\"Write Answers here:\"):\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(user_input)\n",
        "        if user_input:\n",
        "# with st.form(\"Chat\", clear_on_submit=True):\n",
        "#   user_input = st.text_input(\"Write Answers here:\")\n",
        "#   submitted = st.form_submit_button(\"Enter\")\n",
        "#   if submitted and user_input:\n",
        "          st.session_state.conversation_history.append({\"role\":\"system\", \"content\":\"You are an expert pysician taking history from a new patient. You have to ask more questions to get full details of the illness. Gather the information systematically in a step by step fashion\"})\n",
        "\n",
        "          st.session_state.conversation_history.append({\"role\":\"user\", \"content\":user_input})\n",
        "\n",
        "          result = agent.invoke({\"messages\": st.session_state.conversation_history, \"system_questions\": st.session_state.questions, \"running_notes\": \"\",\"current_question\":\"\", \"patient_response\":\"\"},)\n",
        "          conversation_history = result['messages']\n",
        "\n",
        "          st.caption(f\"{result[-1]['plan']}\")\n",
        "        # user_input = input(\"Enter: \")\n",
        "          user_input = st.text_area(\"Write Answers here:\", height=180)\n",
        "          st.session_state.latest_patient_response.append(user_input)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from typing import TypedDict, List, Dict, Annotated, Sequence\n",
        "# from  langgraph.graph import StateGraph, START, END\n",
        "# import torch\n",
        "# from langchain_core.messages import BaseMessage, ToolMessage, SystemMessage\n",
        "# from langchain_core.tools import tool\n",
        "# from langgraph.graph.message import add_messages\n",
        "# from langgraph.prebuilt import ToolNode\n",
        "# from transformers import pipeline\n",
        "# from google.colab import userdata\n",
        "# HF_TOKEN = userdata.get(\"HF_TOKEN\")\n"
      ],
      "metadata": {
        "id": "r1srudbMMPgI"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install localtunnel\n",
        "# !npm audit fix --force"
      ],
      "metadata": {
        "id": "EVuF80yaSUGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb3d5b55-692e-4aed-a1d0-a81cde857846"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 755ms\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K\n",
            "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import urllib.request\n",
        "# print(urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip())"
      ],
      "metadata": {
        "id": "DJJ2JK9d5d_L"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & npx localtunnel --port 8501 & curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNhrcbq51QdK",
        "outputId": "684dae99-a978-4e33-caf5-1b7db0a61b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.143.183.28\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://sixty-seas-hammer.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !streamlit run /usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py"
      ],
      "metadata": {
        "id": "6cJk5yB3lwgj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, List, Dict, Union\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "# from langchain_openai import ChatOpenAI\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "# from dotenv import load_dotenv\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "# from google.colab import userdata\n",
        "# HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "\n",
        "\n",
        "def get_patient_info(patient_id):\n",
        "  # conn = sqlite3.connect(\"patient_clients.db\")\n",
        "  # cursor = conn.cursor()\n",
        "\n",
        "  # cursor.execute(\"SELECT * FROM bio WHERE patient_id = 'patient_id', (50,) \")\n",
        "  # row = cursor.fetchall()\n",
        "  row = \"35 year old Male\"\n",
        "\n",
        "  # conn.close()\n",
        "  return row\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "latest_patient_response = []\n",
        "current_question = []\n",
        "counter = 0\n",
        "patient_id = [1]\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: List[Dict]\n",
        "  running_notes: str\n",
        "  current_question: str\n",
        "  patient_response: str\n",
        "  system_questions: List[Dict]\n",
        "  differentials_list: str\n",
        "  # more_info: bool\n",
        "\n",
        "def get_profile_info(state: AgentState)->AgentState:\n",
        "  \"\"\"This function is a node that gets the patient demographic information and medical history from the database.\"\"\"\n",
        "  pt_info = get_patient_info(patient_id[0])\n",
        "  state['messages'].append({\"role\":\"user\", \"content\":pt_info})\n",
        "  state['running_notes'] = pt_info\n",
        "  return state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def decide_questions_to_ask(state: AgentState)->AgentState:\n",
        "  \"\"\"This function decides which question to ask using an LLM\"\"\"\n",
        "  response = generator(state['messages'], max_new_tokens=256)[0][\"generated_text\"][-1][\"content\"]\n",
        "  state['system_questions'].append({response})\n",
        "  state['current_question'] = response\n",
        "  return state\n",
        "\n",
        "\n",
        "def ask_question_get_answer(state: AgentState)->AgentState:\n",
        "  \"\"\"This funtion asks the questions to the patient and gets the answers\"\"\"\n",
        "  # patient_response = st.text_area(\"Write you answers here:\", height=180, placeholder=\"Your answers here: \")\n",
        "  # latest_patient_response.append(patient_response)\n",
        "  if len(latest_patient_response)>0:\n",
        "    state['patient_response'] = latest_patient_response[-1]\n",
        "    latest_patient_response.pop(0)\n",
        "  return state\n",
        "\n",
        "\n",
        "def compile_notes(state: AgentState)->AgentState:\n",
        "  \"\"\"This combines the current medical history available in the current notes with the answer that the patient has just provided\"\"\"\n",
        "  compile_notes_prompt = f\"As an expert physician that pays attention to detail. Combine the following history {state['running_notes']} and these answers {state['patient_response']} provided by the patient to these questions {state['current_question']}\"\n",
        "  state['running_notes'] = generator([{\"role\":\"user\", \"content\": compile_notes_prompt}], max_new_tokens=256)[0][\"generated_text\"][-1][\"content\"]\n",
        "  return state\n",
        "\n",
        "\n",
        "def is_more_info_needed(state: AgentState)->bool:\n",
        "  \"\"\"Decide if this info is needed to be able to make a good plan for the patient\"\"\"\n",
        "  global counter\n",
        "  prompt_full_history = f\"Answer with 'Yes' or 'No' only. Is this medical history detailed enough to make a good differential diagnosis and then a plan? {state['running_notes']}\"\n",
        "  if counter > 10:\n",
        "    return \"No\"\n",
        "  else:\n",
        "    counter += 1\n",
        "    decision = generator([{\"role\":\"user\", \"content\": prompt_full_history}], max_new_tokens=256)[0][\"generated_text\"][-1][\"content\"]\n",
        "    print(f\"This is the decision on whether more information is needed: {decision}\")\n",
        "    if decision.lower() is True:\n",
        "      return \"Yes\"\n",
        "    else:\n",
        "      return \"No\"\n",
        "\n",
        "def list_possible_differentials(state: AgentState)->AgentState:\n",
        "  \"\"\"This function creates a list of differential diagnosis based on the list\"\"\"\n",
        "  system_prompt_for_differential = f\"You are an expert physician, you have to provide a differential diagnosis for the provided medical history\"\n",
        "  user_prompt_for_differential = state['running_notes']\n",
        "  state['differentials_list'] = generator([{\"role\": \"system\", \"content\":system_prompt_for_differential},{\"role\":\"user\", \"content\":user_prompt_for_differential}], max_new_tokens=256)[0][\"generated_text\"][-1][\"content\"]\n",
        "  return state\n",
        "\n",
        "\n",
        "def create_plan(state: AgentState)->AgentState:\n",
        "  # Need RAG with a directory of physicians\n",
        "  \"\"\"This function creates a plan based on the history and the differential diagnosis list\"\"\"\n",
        "  system_prompt_for_plan = f\"You have to provide medical advise to this individual. Always insist that the patient needs to seek medical advice. Advise which specialty the patient needs to see. Do not provide definitive medical treatment but suggest what the patient should expect when they go to see a physician\"\n",
        "  user_prompt_for_plan = state['running_notes'] + state[\"differentials_list\"]\n",
        "  state['plan'] = generator([{\"role\": \"system\", \"content\":system_prompt_for_plan},{\"role\":\"user\", \"content\":user_prompt_for_plan}], max_new_tokens=256)[0][\"generated_text\"][-1][\"content\"]\n",
        "  return state\n",
        "\n",
        "graph = StateGraph(AgentState)\n",
        "graph.add_node(\"get_profile_info\", get_profile_info)\n",
        "graph.add_node(\"decide_questions_to_ask\", decide_questions_to_ask)\n",
        "graph.add_node(\"ask_question_get_answer\", ask_question_get_answer)\n",
        "graph.add_node(\"compile_notes\", compile_notes)\n",
        "graph.add_node(\"is_more_info_needed\", lambda state:state)\n",
        "graph.add_node(\"list_possible_differentials\", list_possible_differentials)\n",
        "graph.add_node(\"create_plan\", create_plan)\n",
        "\n",
        "\n",
        "graph.add_edge(START, \"get_profile_info\")\n",
        "graph.add_edge(\"get_profile_info\", \"decide_questions_to_ask\")\n",
        "graph.add_edge(\"decide_questions_to_ask\", \"ask_question_get_answer\")\n",
        "graph.add_edge(\"ask_question_get_answer\", \"compile_notes\")\n",
        "graph.add_edge(\"compile_notes\", \"is_more_info_needed\")\n",
        "graph.add_conditional_edges(\n",
        "    \"is_more_info_needed\",\n",
        "    is_more_info_needed,\n",
        "    {\n",
        "       \"Yes\": \"decide_questions_to_ask\",\n",
        "       \"No\": \"list_possible_differentials\",\n",
        "    }\n",
        ")\n",
        "\n",
        "graph.add_edge(\"list_possible_differentials\", \"create_plan\")\n",
        "graph.add_edge(\"create_plan\", END)\n",
        "agent = graph.compile()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(agent.get_graph().draw_mermaid_png()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509,
          "referenced_widgets": [
            "a4679a0023a64ebeb463b5e1a200f63c",
            "6ab8f59d3ecd434496bc150f381fedf3",
            "408ae46a93ce4d5495b16bdd8d1b49b4",
            "5201d563096341a9be104a0727edc282",
            "1664b1799c1844e482854b7b41af674f",
            "8a5074f1605243edafd00eda731a44fd",
            "35b24d75e7354923bbbdfabe2737475b",
            "0a823df219a54315859de2b553115e9b",
            "a3e6b08ca5ac4bf69570bf8b7aa92f0e",
            "06dbfd9722f548b8a928bed2613d806e",
            "48e4e89cfa8045ffb4d8d6c4ae05dfa0"
          ]
        },
        "id": "18CkaWbLefYz",
        "outputId": "77ba8df7-e6e7-453d-e8ca-087461b3dab5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4679a0023a64ebeb463b5e1a200f63c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Failed to reach https://mermaid.ink API while trying to render your graph. Status code: 400.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2888438598.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_mermaid_png\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/graph.py\u001b[0m in \u001b[0;36mdraw_mermaid_png\u001b[0;34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config, base_url, proxies)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0mfrontmatter_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrontmatter_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         )\n\u001b[0;32m--> 693\u001b[0;31m         return draw_mermaid_png(\n\u001b[0m\u001b[1;32m    694\u001b[0m             \u001b[0mmermaid_syntax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmermaid_syntax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0moutput_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_file_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/graph_mermaid.py\u001b[0m in \u001b[0;36mdraw_mermaid_png\u001b[0;34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, base_url, proxies)\u001b[0m\n\u001b[1;32m    310\u001b[0m         )\n\u001b[1;32m    311\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdraw_method\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMermaidDrawMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPI\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         img_bytes = _render_mermaid_using_api(\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0mmermaid_syntax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0moutput_file_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_file_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/graph_mermaid.py\u001b[0m in \u001b[0;36m_render_mermaid_using_api\u001b[0;34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay, proxies, base_url)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;34mf\"your graph. Status code: {response.status_code}.\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             ) + error_msg_suffix\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to reach https://mermaid.ink API while trying to render your graph. Status code: 400.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TO-DO\n",
        "1. Adjust database querries\n",
        "2. Implement safety filters\n",
        "3. Create good GUI\n",
        "4. Display Running Conversation on screen\n",
        "5. Implement and API\n",
        "6. Dockerize\n",
        "7. Create Tests\n",
        "\n",
        "  a. Unit Tests with pytest\n",
        "\n",
        "  b. Safety Tests\n",
        "\n",
        "    i. Regex,\n",
        "\n",
        "  c. Prompt Injection attacks\n",
        "  \n",
        "8. GitHub Actions\n",
        "9. Implement RAG for referring patients, Guidelines e.g. EDLIZ, History Taking Checklist"
      ],
      "metadata": {
        "id": "tKhqq86y1npx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0B3DJ2gz1rwF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}