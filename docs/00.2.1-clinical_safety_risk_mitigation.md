# Making Safe Chatbots for patients in healthcare

This document outlines the steps to be implemented to ensure that Artificial Intelligence (AI) chat agent platforms that interact with patients are safe, reliable and secure. The technology that is used in modern day chatbots is generative AI in the form of Large Language Models (LLMs). These generative technologies produce content in stochastic manner based on their training data. This often leads to the production of incorrect information. These are referred to as hallucinations. There are other challenges with LLMs. These include the risk of accidental disclosure of confidential information, sycophants (which is an attempt by the AI to please user at all costs), limited context window, vulnerability to prompt injection attacks and harmful content generation. 

To address these challenges, it is important to ensure that the design of the agent addresses these concerns.

## ADDRESSING HALLUCINATIONS
To ensure that the information produced by the chatbot is factual and is only influenced by relevant information it is to recheck the generated text for hallucinations. There are many ways to implement this. Here are some tools that should be used to prevent hallucinations.

1. Text Classification
2. RAG
3. LLM
4. Human in the Loop 
5. Fine Tuning
6. Chain of Thought
7. Monitoring 

## PRIVACY PRESERVATION

1. Filters
    a. Name Entity Recognition 

CONTEXT WINDOW
1. Summarization

## PROMPT INJECTION PREVENTION

1. Filters
    a. Regex
    b. LLM

## TRANSPARENCY CHECKLIST

## MODEL CARD
